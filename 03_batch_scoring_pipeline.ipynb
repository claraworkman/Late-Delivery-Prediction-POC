{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649cec3d",
   "metadata": {},
   "source": [
    "# Batch Scoring Pipeline - Ship Date Predictions for Open Deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc91028",
   "metadata": {},
   "source": [
    "**Goal:** Generate ship date predictions for **open deliveries** (not yet shipped).\n",
    "\n",
    "**Use Case:** Enable operations team to:\n",
    "- Forecast when orders will ship from the distribution center\n",
    "- Plan logistics and inventory allocation\n",
    "- Set realistic customer expectations for ship dates\n",
    "- Identify orders with unusually long predicted processing times\n",
    "\n",
    "**Workflow:**\n",
    "1. Load trained regression model from MLflow\n",
    "2. Get **open deliveries** using DAX (deliveries without GI Date)\n",
    "3. Generate predictions: days to ship + predicted ship date\n",
    "4. Save predictions to Lakehouse table: `ship_date_predictions`\n",
    "5. Visualize predicted ship dates\n",
    "6. Enable Power BI reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeba6fc",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# ==============================================================================\n",
    "# WHY: Load libraries needed to:\n",
    "#      1. Load the trained model from MLflow\n",
    "#      2. Query semantic model for open deliveries\n",
    "#      3. Generate predictions\n",
    "#      4. Save results to Lakehouse\n",
    "# ==============================================================================\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration - UPDATE to match your environment\n",
    "DATASET = \"DLV Aging Columns & Measures\"  # Your semantic model name\n",
    "MODEL_NAME = \"ship_date_predictor\"        # Model from Notebook 02\n",
    "TARGET_COLUMN = \"DAYS_TO_SHIP\"            # What we're predicting\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Semantic Model: {DATASET}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Target: {TARGET_COLUMN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78817577",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TRAINED MODEL FROM MLFLOW\n",
    "# ==============================================================================\n",
    "# WHY: Retrieve the model we trained in Notebook 02.\n",
    "#      MLflow stores model versions and metadata.\n",
    "#\n",
    "# Model loading process:\n",
    "# 1. Search for latest version of \"ship_date_predictor\"\n",
    "# 2. Load the model artifact\n",
    "# 3. Verify it's ready for predictions\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading trained model from MLflow...\")\n",
    "\n",
    "# Load the latest production model\n",
    "model_uri = f\"models:/{MODEL_NAME}/latest\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"âœ… Model loaded: {MODEL_NAME}\")\n",
    "print(f\"âœ… Model type: {type(model).__name__}\")\n",
    "print(f\"âœ… Ready for predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ef4e4",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 3. Load Trained Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from MLflow registry\n",
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_VERSION}\"\n",
    "\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"   Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ecf66",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 4. Load Open Deliveries from Semantic Model\n",
    "\n",
    "Load **only open deliveries** (not yet shipped) that need predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD OPEN DELIVERIES (TO BE SCORED)\n",
    "# ==============================================================================\n",
    "# WHY: These are deliveries awaiting shipment - we want to predict when they'll ship.\n",
    "#\n",
    "# FILTER: ISBLANK(Aging[GI Date]) AND NOT(ISBLANK(Aging[Delivery Created On]))\n",
    "#         This selects deliveries that have NOT shipped yet (no GI Date).\n",
    "#         We also need Delivery Created On to calculate the predicted ship date.\n",
    "#\n",
    "# OPPOSITE of Notebook 02: Training used closed (shipped), scoring uses open (not shipped)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading open deliveries from semantic model...\")\n",
    "\n",
    "ws = fabric.get_workspace_id()\n",
    "\n",
    "# DAX query for OPEN deliveries (no GI Date = not shipped yet)\n",
    "dax_query = \"\"\"\n",
    "EVALUATE\n",
    "FILTER(\n",
    "    Aging,\n",
    "    ISBLANK(Aging[GI Date]) &&\n",
    "    NOT(ISBLANK(Aging[Delivery Created On]))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df_open = fabric.evaluate_dax(dataset=DATASET, dax_string=dax_query, workspace=ws)\n",
    "\n",
    "# Clean column names\n",
    "df_open.columns = [col.split('[')[-1].replace(']', '') if '[' in col else col for col in df_open.columns]\n",
    "\n",
    "print(f\"âœ… Loaded {len(df_open):,} open deliveries (awaiting shipment)\")\n",
    "print(f\"âœ… Columns: {df_open.shape[1]}\")\n",
    "df_open.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b0dda",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 5. Prepare Features\n",
    "\n",
    "Extract the same features used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714836e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (must match training notebook)\n",
    "potential_features = [\n",
    "    'Plant', 'Shipping Point', 'EWM Carrier Code',\n",
    "    'Brand', 'Channel', 'Product Category', 'Product Type', 'Standard Or Custom',\n",
    "    'STRATEGIC_ACCOUNT', 'Sold To - Key',\n",
    "    'Delivery Type', 'DELIVERY_QTY', 'DELIVERY_VALUE_USD', 'Delivery Priority', 'Shipping Condition',\n",
    "    'Credit Status', 'Distribution Status', 'STATUS'\n",
    "]\n",
    "\n",
    "# Add temporal features\n",
    "if 'Delivery Created On' in df_open.columns:\n",
    "    try:\n",
    "        df_open['created_dayofweek'] = pd.to_datetime(df_open['Delivery Created On']).dt.dayofweek\n",
    "        df_open['created_month'] = pd.to_datetime(df_open['Delivery Created On']).dt.month\n",
    "        potential_features.extend(['created_dayofweek', 'created_month'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Filter to available features\n",
    "feature_cols = [f for f in potential_features if f in df_open.columns]\n",
    "print(f\"âœ… Using {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6b45f",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 6. Feature Engineering\n",
    "\n",
    "Apply the same transformations used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PREPARE FEATURES (MUST MATCH TRAINING)\n",
    "# ==============================================================================\n",
    "# WHY: The model was trained on specific features in a specific format.\n",
    "#      We must prepare the scoring data EXACTLY the same way.\n",
    "#\n",
    "# Critical: Use the SAME feature list and encoding as Notebook 02!\n",
    "# ==============================================================================\n",
    "\n",
    "# Same feature list as training (from Notebook 02)\n",
    "# IMPORTANT: This must match exactly what was used in training\n",
    "feature_cols = [\n",
    "    'Plant', 'Shipping Point', 'EWM Carrier Code',\n",
    "    'Brand', 'Channel', 'Product Category', 'Product Type', 'Standard Or Custom',\n",
    "    'STRATEGIC_ACCOUNT', 'Sold To - Key',\n",
    "    'Delivery Type', 'DELIVERY_QTY', 'DELIVERY_VALUE_USD', 'Delivery Priority', 'Shipping Condition',\n",
    "    'Credit Status', 'Distribution Status', 'STATUS'\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the data\n",
    "available_features = [f for f in feature_cols if f in df_open.columns]\n",
    "\n",
    "print(f\"=== Feature Matching ===\")\n",
    "print(f\"Expected features: {len(feature_cols)}\")\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "\n",
    "if len(available_features) < len(feature_cols):\n",
    "    missing = [f for f in feature_cols if f not in df_open.columns]\n",
    "    print(f\"âš ï¸ Missing features: {missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77c30",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENCODE FEATURES (SAME AS TRAINING)\n",
    "# ==============================================================================\n",
    "# WHY: Convert text to numbers using the SAME method as Notebook 02.\n",
    "#      The model expects numeric inputs in the exact same format.\n",
    "#\n",
    "# Steps: Text â†’ \"Unknown\" for missing â†’ Category codes â†’ Fill numeric NaNs\n",
    "# ==============================================================================\n",
    "\n",
    "X_open = df_open[available_features].copy()\n",
    "\n",
    "# Encode categorical variables (text â†’ numbers)\n",
    "categorical_cols = X_open.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    X_open[col] = X_open[col].fillna('Unknown')\n",
    "    X_open[col] = X_open[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Handle numeric NaNs\n",
    "numeric_cols = X_open.select_dtypes(include=['number']).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if X_open[col].isnull().sum() > 0:\n",
    "        X_open[col] = X_open[col].fillna(X_open[col].median())\n",
    "\n",
    "print(f\"âœ… Features prepared: {X_open.shape[1]} columns, {X_open.shape[0]:,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87327e",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 8. Create Output Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b63c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE PREDICTIONS\n",
    "# ==============================================================================\n",
    "# WHY: Use the trained model to predict how many days from creation to ship.\n",
    "#\n",
    "# Output: For each open delivery, we get:\n",
    "# - predicted_days_to_ship: Numeric prediction (e.g., 5 = will ship in 5 days)\n",
    "# - predicted_ship_date: Delivery Created On + predicted_days_to_ship\n",
    "# - lead_time_bucket: Category (0-2, 3-5, 6-9, 10+ days)\n",
    "# - extended_processing: Yes/No flag for unusually long processing times\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING SHIP DATE PREDICTIONS FOR OPEN DELIVERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_open)\n",
    "\n",
    "print(f\"âœ… Generated {len(predictions):,} predictions\")\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_open['predicted_days_to_ship'] = predictions\n",
    "\n",
    "# Calculate predicted ship date\n",
    "# WHY: Convert days-to-ship into an actual date for business users\n",
    "df_open['predicted_ship_date'] = pd.to_datetime(df_open['Delivery Created On']) + pd.to_timedelta(df_open['predicted_days_to_ship'], unit='d')\n",
    "\n",
    "# Create lead time buckets for categorization\n",
    "# WHY: Business teams can group deliveries by expected processing time\n",
    "def categorize_lead_time(days):\n",
    "    if days <= 2:\n",
    "        return \"0-2 days\"\n",
    "    elif days <= 5:\n",
    "        return \"3-5 days\"\n",
    "    elif days <= 9:\n",
    "        return \"6-9 days\"\n",
    "    else:\n",
    "        return \"10+ days\"\n",
    "\n",
    "df_open['lead_time_bucket'] = df_open['predicted_days_to_ship'].apply(categorize_lead_time)\n",
    "\n",
    "# Flag extended processing deliveries\n",
    "# WHY: Strategic accounts + predicted >7 days = need attention\n",
    "df_open['extended_processing'] = (\n",
    "    (df_open.get('STRATEGIC_ACCOUNT', '') == 'Yes') & \n",
    "    (df_open['predicted_days_to_ship'] > 7)\n",
    ").apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "print(\"\\n=== Prediction Summary ===\")\n",
    "print(f\"Average predicted days to ship: {predictions.mean():.2f} days\")\n",
    "print(f\"Max predicted days to ship: {predictions.max():.2f} days\")\n",
    "print(f\"Extended processing deliveries: {(df_open['extended_processing'] == 'Yes').sum():,}\")\n",
    "print(\"\\nLead time distribution:\")\n",
    "print(df_open['lead_time_bucket'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8376de0",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 9. Save Predictions to Lakehouse\n",
    "\n",
    "Write predictions to a Delta table for Power BI consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Lakehouse\n",
    "spark_df = spark.createDataFrame(output_df)\n",
    "\n",
    "print(f\"ðŸ’¾ Writing to Lakehouse table: {OUTPUT_TABLE}\")\n",
    "\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(f\"âœ… Saved {len(output_df):,} predictions\")\n",
    "print(f\"   Predicted LATE: {output_df['is_late'].sum():,}\")\n",
    "print(f\"   Predicted ON-TIME: {(~output_df['is_late'].astype(bool)).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd2e79",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE PREDICTIONS TO LAKEHOUSE\n",
    "# ==============================================================================\n",
    "# WHY: Save predictions to a Lakehouse table so Power BI can consume them.\n",
    "#\n",
    "# Table: ship_date_predictions\n",
    "# Mode: Overwrite (replaces old predictions each time)\n",
    "#\n",
    "# Next Step: Add this table to your Power BI semantic model and build dashboards!\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n=== Saving Predictions to Lakehouse ===\")\n",
    "\n",
    "# Select key columns for output\n",
    "output_cols = [\n",
    "    'Delivery Document',\n",
    "    'Plant',\n",
    "    'Brand',\n",
    "    'Channel',\n",
    "    'EWM Carrier Code',\n",
    "    'STRATEGIC_ACCOUNT',\n",
    "    'DELIVERY_QTY',\n",
    "    'DELIVERY_VALUE_USD',\n",
    "    'Delivery Created On',\n",
    "    'predicted_days_to_ship',\n",
    "    'predicted_ship_date',\n",
    "    'lead_time_bucket',\n",
    "    'extended_processing'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "available_output_cols = [c for c in output_cols if c in df_open.columns]\n",
    "predictions_df = df_open[available_output_cols].copy()\n",
    "\n",
    "# Add prediction timestamp\n",
    "predictions_df['prediction_timestamp'] = datetime.now()\n",
    "\n",
    "# Save to Lakehouse table\n",
    "table_name = \"ship_date_predictions\"\n",
    "spark_df = spark.createDataFrame(predictions_df)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Saved {len(predictions_df):,} predictions to table: {table_name}\")\n",
    "print(f\"âœ… Columns saved: {len(available_output_cols) + 1}\")\n",
    "print(\"\\nðŸ’¡ Next: Add this table to your Power BI semantic model!\")\n",
    "print(\"\\n=== Key Columns ===\")\n",
    "print(f\"  â€¢ predicted_days_to_ship: Days from creation to expected ship\")\n",
    "print(f\"  â€¢ predicted_ship_date: Calculated ship date (Delivery Created On + days)\")\n",
    "print(f\"  â€¢ lead_time_bucket: Processing time category (0-2, 3-5, 6-9, 10+)\")\n",
    "print(f\"  â€¢ extended_processing: Flag for strategic accounts with >7 day lead times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53bc69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Predictions Complete!\n",
    "\n",
    "The `ship_date_predictions` table is now available in your Lakehouse and can be used in Power BI for reporting and dashboards.\n",
    "\n",
    "**Key outputs:**\n",
    "- `predicted_days_to_ship`: Days from creation to expected DC ship date\n",
    "- `predicted_ship_date`: Actual forecasted ship date\n",
    "- `lead_time_bucket`: Processing time grouping\n",
    "- `extended_processing`: Flag for strategic account deliveries with >7 day lead times"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
