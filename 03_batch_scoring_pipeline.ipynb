{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649cec3d",
   "metadata": {},
   "source": [
    "# Batch Scoring Pipeline - Late Delivery Predictions for Open Deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc91028",
   "metadata": {},
   "source": [
    "**Goal:** Generate late delivery risk predictions for **open deliveries** (not yet shipped).\n",
    "\n",
    "**Use Case:** Enable operations team to:\n",
    "- Identify deliveries at high risk of shipping late\n",
    "- Prioritize corrective actions for strategic accounts\n",
    "- Proactively communicate with business teams about potential delays\n",
    "\n",
    "**Workflow:**\n",
    "1. Load trained regression model from MLflow\n",
    "2. Get **open deliveries** using DAX (deliveries without GI Date)\n",
    "3. Generate predictions: days late + risk score + lateness bucket\n",
    "4. Save predictions to Lakehouse table: `late_delivery_predictions`\n",
    "5. Visualize high-risk deliveries\n",
    "6. Enable Power BI reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeba6fc",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IMPORTS & CONFIGURATION\n",
    "# ==============================================================================\n",
    "# WHY: Load libraries needed to:\n",
    "#      1. Load the trained model from MLflow\n",
    "#      2. Query semantic model for open deliveries\n",
    "#      3. Generate predictions\n",
    "#      4. Save results to Lakehouse\n",
    "# ==============================================================================\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration - UPDATE to match your environment\n",
    "DATASET = \"DLV Aging Columns & Measures\"  # Your semantic model name\n",
    "MODEL_NAME = \"late_delivery_predictor\"    # Model from Notebook 02\n",
    "TARGET_COLUMN = \"AGE_REQ_DATE\"             # What we're predicting\n",
    "\n",
    "print(\"âœ… Configuration loaded\")\n",
    "print(f\"   Semantic Model: {DATASET}\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Target: {TARGET_COLUMN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78817577",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD TRAINED MODEL FROM MLFLOW\n",
    "# ==============================================================================\n",
    "# WHY: Retrieve the model we trained in Notebook 02.\n",
    "#      MLflow stores model versions and metadata.\n",
    "#\n",
    "# Model loading process:\n",
    "# 1. Search for latest version of \"late_delivery_predictor\"\n",
    "# 2. Load the model artifact\n",
    "# 3. Verify it's ready for predictions\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading trained model from MLflow...\")\n",
    "\n",
    "# Load the latest production model\n",
    "model_uri = f\"models:/{MODEL_NAME}/latest\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"âœ… Model loaded: {MODEL_NAME}\")\n",
    "print(f\"âœ… Model type: {type(model).__name__}\")\n",
    "print(f\"âœ… Ready for predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ef4e4",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 3. Load Trained Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from MLflow registry\n",
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_VERSION}\"\n",
    "\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"   Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ecf66",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 4. Load Open Deliveries from Semantic Model\n",
    "\n",
    "Load **only open deliveries** (not yet shipped) that need predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD OPEN DELIVERIES (TO BE SCORED)\n",
    "# ==============================================================================\n",
    "# WHY: These are deliveries awaiting shipment - we want to predict if they'll be late.\n",
    "#\n",
    "# FILTER: ISBLANK(Aging[GI Date])\n",
    "#         This selects deliveries that have NOT shipped yet (no GI Date).\n",
    "#         These are the ones we need to predict for!\n",
    "#\n",
    "# OPPOSITE of Notebook 02: Training used closed (shipped), scoring uses open (not shipped)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Loading open deliveries from semantic model...\")\n",
    "\n",
    "ws = fabric.get_workspace_id()\n",
    "\n",
    "# DAX query for OPEN deliveries (no GI Date = not shipped yet)\n",
    "dax_query = \"\"\"\n",
    "EVALUATE\n",
    "FILTER(\n",
    "    Aging,\n",
    "    ISBLANK(Aging[GI Date])\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df_open = fabric.evaluate_dax(dataset=DATASET, dax_string=dax_query, workspace=ws)\n",
    "\n",
    "# Clean column names\n",
    "df_open.columns = [col.split('[')[-1].replace(']', '') if '[' in col else col for col in df_open.columns]\n",
    "\n",
    "print(f\"âœ… Loaded {len(df_open):,} open deliveries (awaiting shipment)\")\n",
    "print(f\"âœ… Columns: {df_open.shape[1]}\")\n",
    "df_open.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b0dda",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 5. Prepare Features\n",
    "\n",
    "Extract the same features used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714836e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (must match training notebook)\n",
    "potential_features = [\n",
    "    'Plant', 'Shipping Point', 'EWM Carrier Code',\n",
    "    'Brand', 'Channel', 'Product Category', 'Product Type', 'Standard Or Custom',\n",
    "    'STRATEGIC_ACCOUNT', 'Sold To - Key',\n",
    "    'Delivery Type', 'DELIVERY_QTY', 'DELIVERY_VALUE_USD', 'Delivery Priority', 'Shipping Condition',\n",
    "    'Credit Status', 'Distribution Status', 'STATUS'\n",
    "]\n",
    "\n",
    "# Add temporal features\n",
    "if 'Delivery Created On' in df_open.columns:\n",
    "    try:\n",
    "        df_open['created_dayofweek'] = pd.to_datetime(df_open['Delivery Created On']).dt.dayofweek\n",
    "        df_open['created_month'] = pd.to_datetime(df_open['Delivery Created On']).dt.month\n",
    "        potential_features.extend(['created_dayofweek', 'created_month'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Filter to available features\n",
    "feature_cols = [f for f in potential_features if f in df_open.columns]\n",
    "print(f\"âœ… Using {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6b45f",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 6. Feature Engineering\n",
    "\n",
    "Apply the same transformations used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PREPARE FEATURES (MUST MATCH TRAINING)\n",
    "# ==============================================================================\n",
    "# WHY: The model was trained on specific features in a specific format.\n",
    "#      We must prepare the scoring data EXACTLY the same way.\n",
    "#\n",
    "# Critical: Use the SAME feature list and encoding as Notebook 02!\n",
    "# ==============================================================================\n",
    "\n",
    "# Same feature list as training (from Notebook 02)\n",
    "# IMPORTANT: This must match exactly what was used in training\n",
    "feature_cols = [\n",
    "    'Plant', 'Shipping Point', 'EWM Carrier Code',\n",
    "    'Brand', 'Channel', 'Product Category', 'Product Type', 'Standard Or Custom',\n",
    "    'STRATEGIC_ACCOUNT', 'Sold To - Key',\n",
    "    'Delivery Type', 'DELIVERY_QTY', 'DELIVERY_VALUE_USD', 'Delivery Priority', 'Shipping Condition',\n",
    "    'Credit Status', 'Distribution Status', 'STATUS'\n",
    "]\n",
    "\n",
    "# Filter to features that exist in the data\n",
    "available_features = [f for f in feature_cols if f in df_open.columns]\n",
    "\n",
    "print(f\"=== Feature Matching ===\")\n",
    "print(f\"Expected features: {len(feature_cols)}\")\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "\n",
    "if len(available_features) < len(feature_cols):\n",
    "    missing = [f for f in feature_cols if f not in df_open.columns]\n",
    "    print(f\"âš ï¸ Missing features: {missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77c30",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ENCODE FEATURES (SAME AS TRAINING)\n",
    "# ==============================================================================\n",
    "# WHY: Convert text to numbers using the SAME method as Notebook 02.\n",
    "#      The model expects numeric inputs in the exact same format.\n",
    "#\n",
    "# Steps: Text â†’ \"Unknown\" for missing â†’ Category codes â†’ Fill numeric NaNs\n",
    "# ==============================================================================\n",
    "\n",
    "X_open = df_open[available_features].copy()\n",
    "\n",
    "# Encode categorical variables (text â†’ numbers)\n",
    "categorical_cols = X_open.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    X_open[col] = X_open[col].fillna('Unknown')\n",
    "    X_open[col] = X_open[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Handle numeric NaNs\n",
    "numeric_cols = X_open.select_dtypes(include=['number']).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if X_open[col].isnull().sum() > 0:\n",
    "        X_open[col] = X_open[col].fillna(X_open[col].median())\n",
    "\n",
    "print(f\"âœ… Features prepared: {X_open.shape[1]} columns, {X_open.shape[0]:,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87327e",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 8. Create Output Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b63c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GENERATE PREDICTIONS\n",
    "# ==============================================================================\n",
    "# WHY: Use the trained model to predict how many days late each open delivery will be.\n",
    "#\n",
    "# Output: For each open delivery, we get:\n",
    "# - predicted_days_late: Numeric prediction (e.g., +7 = 7 days late)\n",
    "# - risk_score: Confidence/severity (0-1 scale)\n",
    "# - lateness_bucket: Category (0-2, 3-5, 6-9, 10+ days late)\n",
    "# - high_priority: Yes/No flag for action\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING PREDICTIONS FOR OPEN DELIVERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_open)\n",
    "\n",
    "print(f\"âœ… Generated {len(predictions):,} predictions\")\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_open['predicted_days_late'] = predictions\n",
    "\n",
    "# Create risk score (0-1 scale based on severity)\n",
    "# WHY: Helps prioritize which deliveries need immediate attention\n",
    "df_open['risk_score'] = np.clip(predictions / 10, 0, 1)  # Normalize to 0-1\n",
    "\n",
    "# Create lateness buckets for categorization\n",
    "# WHY: Business teams can group deliveries by severity\n",
    "def categorize_lateness(days):\n",
    "    if days <= 2:\n",
    "        return \"0-2 days late\"\n",
    "    elif days <= 5:\n",
    "        return \"3-5 days late\"\n",
    "    elif days <= 9:\n",
    "        return \"6-9 days late\"\n",
    "    else:\n",
    "        return \"10+ days late\"\n",
    "\n",
    "df_open['lateness_bucket'] = df_open['predicted_days_late'].apply(categorize_lateness)\n",
    "\n",
    "# Flag high priority deliveries\n",
    "# WHY: Strategic accounts + predicted >5 days late = immediate action needed\n",
    "df_open['high_priority'] = (\n",
    "    (df_open.get('STRATEGIC_ACCOUNT', '') == 'Yes') & \n",
    "    (df_open['predicted_days_late'] > 5)\n",
    ").apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "print(\"\\n=== Prediction Summary ===\")\n",
    "print(f\"Average predicted lateness: {predictions.mean():.2f} days\")\n",
    "print(f\"Max predicted lateness: {predictions.max():.2f} days\")\n",
    "print(f\"High priority deliveries: {(df_open['high_priority'] == 'Yes').sum():,}\")\n",
    "print(\"\\nLateness distribution:\")\n",
    "print(df_open['lateness_bucket'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8376de0",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 9. Save Predictions to Lakehouse\n",
    "\n",
    "Write predictions to a Delta table for Power BI consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Lakehouse\n",
    "spark_df = spark.createDataFrame(output_df)\n",
    "\n",
    "print(f\"ðŸ’¾ Writing to Lakehouse table: {OUTPUT_TABLE}\")\n",
    "\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "print(f\"âœ… Saved {len(output_df):,} predictions\")\n",
    "print(f\"   Predicted LATE: {output_df['is_late'].sum():,}\")\n",
    "print(f\"   Predicted ON-TIME: {(~output_df['is_late'].astype(bool)).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd2e79",
   "metadata": {},
   "source": [
    "### ðŸŸ¦ 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SAVE PREDICTIONS TO LAKEHOUSE\n",
    "# ==============================================================================\n",
    "# WHY: Save predictions to a Lakehouse table so Power BI can consume them.\n",
    "#\n",
    "# Table: late_delivery_predictions\n",
    "# Mode: Overwrite (replaces old predictions each time)\n",
    "#\n",
    "# Next Step: Add this table to your Power BI semantic model and build dashboards!\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n=== Saving Predictions to Lakehouse ===\")\n",
    "\n",
    "# Select key columns for output\n",
    "output_cols = [\n",
    "    'Delivery Document',\n",
    "    'Plant',\n",
    "    'Brand',\n",
    "    'Channel',\n",
    "    'EWM Carrier Code',\n",
    "    'STRATEGIC_ACCOUNT',\n",
    "    'DELIVERY_QTY',\n",
    "    'DELIVERY_VALUE_USD',\n",
    "    'Customer Requested Delivery Date',\n",
    "    'predicted_days_late',\n",
    "    'risk_score',\n",
    "    'lateness_bucket',\n",
    "    'high_priority'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "available_output_cols = [c for c in output_cols if c in df_open.columns]\n",
    "predictions_df = df_open[available_output_cols].copy()\n",
    "\n",
    "# Add prediction timestamp\n",
    "predictions_df['prediction_timestamp'] = datetime.now()\n",
    "\n",
    "# Save to Lakehouse table\n",
    "table_name = \"late_delivery_predictions\"\n",
    "spark_df = spark.createDataFrame(predictions_df)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"âœ… Saved {len(predictions_df):,} predictions to table: {table_name}\")\n",
    "print(f\"âœ… Columns saved: {len(available_output_cols) + 1}\")\n",
    "print(\"\\nðŸ’¡ Next: Add this table to your Power BI semantic model!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53bc69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Predictions Complete!\n",
    "\n",
    "The `late_delivery_predictions` table is now available in your Lakehouse and can be used in Power BI for reporting and dashboards."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
