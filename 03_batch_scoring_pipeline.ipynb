{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649cec3d",
   "metadata": {},
   "source": [
    "# Batch Scoring Pipeline - Late Delivery Predictions for Open Deliveries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc91028",
   "metadata": {},
   "source": [
    "**Goal:** Generate late delivery risk predictions for **open deliveries** (not yet shipped).\n",
    "\n",
    "**Use Case:** Enable operations team to:\n",
    "- Identify deliveries at high risk of shipping late\n",
    "- Prioritize corrective actions for strategic accounts\n",
    "- Proactively communicate with business teams about potential delays\n",
    "\n",
    "**Workflow:**\n",
    "1. Load trained regression model from MLflow\n",
    "2. Get **open deliveries** using DAX (deliveries without GI Date)\n",
    "3. Generate predictions: days late + risk score + lateness bucket\n",
    "4. Save predictions to Lakehouse table: `late_delivery_predictions`\n",
    "5. Visualize high-risk deliveries\n",
    "6. Enable Power BI reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeba6fc",
   "metadata": {},
   "source": [
    "### üü¶ 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model loading\n",
    "import mlflow\n",
    "\n",
    "# Semantic Link - Connect to Power BI\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "# Spark for writing to Lakehouse\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78817577",
   "metadata": {},
   "source": [
    "### üü¶ 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic model name\n",
    "DATASET = \"DLV Aging Columns & Measures\"\n",
    "\n",
    "# MLflow model details\n",
    "MODEL_NAME = \"POC-LateDelivery-Regression-AutoML\"\n",
    "MODEL_VERSION = \"latest\"  # or specify version like \"1\", \"2\", etc.\n",
    "\n",
    "# Target variable (must match training notebook)\n",
    "TARGET_COLUMN = \"AGE_REQ_DATE\"\n",
    "\n",
    "# Output table name\n",
    "OUTPUT_TABLE = \"late_delivery_predictions\"\n",
    "\n",
    "# Workspace\n",
    "ws = fabric.get_workspace_id()\n",
    "\n",
    "print(f\"üìä Dataset: {DATASET}\")\n",
    "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
    "print(f\"üìå Version: {MODEL_VERSION}\")\n",
    "print(f\"üéØ Target: {TARGET_COLUMN}\")\n",
    "print(f\"üíæ Output Table: {OUTPUT_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ef4e4",
   "metadata": {},
   "source": [
    "### üü¶ 3. Load Trained Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from MLflow registry\n",
    "model_uri = f\"models:/{MODEL_NAME}/{MODEL_VERSION}\"\n",
    "\n",
    "print(f\"Loading model from: {model_uri}\")\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ecf66",
   "metadata": {},
   "source": [
    "### üü¶ 4. Load Open Deliveries from Semantic Model\n",
    "\n",
    "Load **only open deliveries** (not yet shipped) that need predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAX query: Get OPEN deliveries (no GI Date = not yet shipped)\n",
    "# These are the deliveries we want to predict late delivery risk for\n",
    "dax_query = \"\"\"\n",
    "EVALUATE\n",
    "FILTER(\n",
    "    Aging,\n",
    "    ISBLANK(Aging[GI Date])\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Execute DAX query\n",
    "print(\"Loading open deliveries from semantic model...\")\n",
    "df_open = fabric.evaluate_dax(dataset=DATASET, dax_string=dax_query, workspace=ws)\n",
    "\n",
    "# Clean column names\n",
    "df_open.columns = [col.split('[')[-1].replace(']', '') if '[' in col else col for col in df_open.columns]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_open):,} open deliveries (awaiting shipment)\")\n",
    "print(f\"‚úÖ Columns: {df_open.shape[1]}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nSample open deliveries:\")\n",
    "df_open[['Delivery Number', 'Plant', 'Brand', 'Channel', 'STRATEGIC_ACCOUNT', \n",
    "         'Delivery Created On', 'Req. Date Header', 'STATUS']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b0dda",
   "metadata": {},
   "source": [
    "### üü¶ 5. Prepare Features\n",
    "\n",
    "Extract the same features used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034b27c",
   "metadata": {},
   "source": [
    "**IMPORTANT:** These features must match exactly what was used in notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714836e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features - MUST MATCH TRAINING NOTEBOOK\n",
    "potential_features = [\n",
    "    # Location & Routing\n",
    "    'Plant',\n",
    "    'Shipping Point',\n",
    "    'EWM_CARRIER_CODE',\n",
    "    \n",
    "    # Product\n",
    "    'Brand',\n",
    "    'Channel',\n",
    "    'Product Category',\n",
    "    'Product Type',\n",
    "    'Standard Or Custom',\n",
    "    \n",
    "    # Customer & Account\n",
    "    'STRATEGIC_ACCOUNT',\n",
    "    'Sold To - Key',\n",
    "    \n",
    "    # Delivery Attributes\n",
    "    'Delivery Type',\n",
    "    'DELIVERY_QTY',\n",
    "    'DELIVERY_VALUE_USD',\n",
    "    'Delivery Priority',\n",
    "    'Shipping Condition',\n",
    "    \n",
    "    # Processing Status\n",
    "    'Credit Status',\n",
    "    'Distribution Status',\n",
    "    'STATUS',\n",
    "]\n",
    "\n",
    "# Add temporal features if 'Delivery Created On' exists\n",
    "if 'Delivery Created On' in df_open.columns:\n",
    "    try:\n",
    "        df_open['created_dayofweek'] = pd.to_datetime(df_open['Delivery Created On']).dt.dayofweek\n",
    "        df_open['created_month'] = pd.to_datetime(df_open['Delivery Created On']).dt.month\n",
    "        potential_features.extend(['created_dayofweek', 'created_month'])\n",
    "        print(\"‚úÖ Added temporal features\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not create temporal features: {e}\")\n",
    "\n",
    "# Filter to available features\n",
    "feature_cols = [f for f in potential_features if f in df_open.columns]\n",
    "\n",
    "print(f\"=== Features for Scoring ===\")\n",
    "print(f\"Using {len(feature_cols)} features:\")\n",
    "for i, f in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {f}\")\n",
    "\n",
    "missing_features = [f for f in potential_features if f not in df_open.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing features: {missing_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6b45f",
   "metadata": {},
   "source": [
    "### üü¶ 6. Feature Engineering\n",
    "\n",
    "Apply the same transformations used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad3d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = df_open[feature_cols].copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = X.select_dtypes(include=['object', 'string']).columns.tolist()\n",
    "\n",
    "print(f\"\\n=== Encoding Categorical Features ===\")\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].fillna('Unknown')\n",
    "    X[col] = X[col].astype(\"category\").cat.codes\n",
    "    print(f\"  ‚úì Encoded: {col}\")\n",
    "\n",
    "# Handle numeric NaNs\n",
    "numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if X[col].isnull().sum() > 0:\n",
    "        median_val = X[col].median()\n",
    "        X[col] = X[col].fillna(median_val)\n",
    "        print(f\"  ‚úì Filled NaNs in {col} with median: {median_val}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Features prepared: {X.shape[1]} columns, {X.shape[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b77c30",
   "metadata": {},
   "source": [
    "### üü¶ 7. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"üîÆ Generating late delivery predictions...\")\n",
    "predicted_days_late = model.predict(X)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(predicted_days_late):,} predictions\")\n",
    "print(f\"\\nPredicted AGE_REQ_DATE statistics:\")\n",
    "print(f\"  Mean: {predicted_days_late.mean():.2f} days\")\n",
    "print(f\"  Median: {np.median(predicted_days_late):.2f} days\")\n",
    "print(f\"  Min: {predicted_days_late.min():.2f} days\")\n",
    "print(f\"  Max: {predicted_days_late.max():.2f} days\")\n",
    "print(f\"  Std Dev: {predicted_days_late.std():.2f} days\")\n",
    "\n",
    "# Calculate late delivery metrics\n",
    "late_count = (predicted_days_late > 0).sum()\n",
    "on_time_count = (predicted_days_late <= 0).sum()\n",
    "\n",
    "print(f\"\\nüìä Late Delivery Forecast:\")\n",
    "print(f\"  Predicted LATE: {late_count:,} ({late_count/len(predicted_days_late)*100:.1f}%)\")\n",
    "print(f\"  Predicted ON-TIME/EARLY: {on_time_count:,} ({on_time_count/len(predicted_days_late)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87327e",
   "metadata": {},
   "source": [
    "### üü¶ 8. Create Output DataFrame with Risk Scores and Buckets\n",
    "\n",
    "Combine predictions with business logic for late delivery risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b63c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create output dataframe with predictions\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "# Add key identifiers\n",
    "if 'Delivery Number' in df_open.columns:\n",
    "    output_df['Delivery_Number'] = df_open['Delivery Number']\n",
    "\n",
    "# Add business-critical columns\n",
    "key_columns = ['Plant', 'Brand', 'Channel', 'Product Category', 'STRATEGIC_ACCOUNT',\n",
    "               'Sales Order', 'Delivery Created On', 'Req. Date Header', \n",
    "               'DELIVERY_QTY', 'DELIVERY_VALUE_USD', 'STATUS',\n",
    "               'EWM_CARRIER_CODE', 'Credit Status', 'Distribution Status']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in df_open.columns:\n",
    "        col_name = col.replace(' ', '_').replace('.', '')\n",
    "        output_df[col_name] = df_open[col]\n",
    "\n",
    "# Add predictions\n",
    "output_df['predicted_days_late'] = predicted_days_late\n",
    "\n",
    "# Add classification: is_late (binary)\n",
    "output_df['is_late'] = (predicted_days_late > 0).astype(int)\n",
    "output_df['is_late_label'] = output_df['is_late'].map({1: 'LATE', 0: 'ON-TIME'})\n",
    "\n",
    "# Add lateness buckets (matching use case requirements: 0-2, 3-5, 6-9, 10+)\n",
    "def categorize_lateness(days):\n",
    "    if days <= 0:\n",
    "        return 'On-time/Early'\n",
    "    elif days <= 2:\n",
    "        return '0-2 days late'\n",
    "    elif days <= 5:\n",
    "        return '3-5 days late'\n",
    "    elif days <= 9:\n",
    "        return '6-9 days late'\n",
    "    else:\n",
    "        return '10+ days late'\n",
    "\n",
    "output_df['lateness_bucket'] = predicted_days_late.apply(categorize_lateness)\n",
    "\n",
    "# Add risk score (normalized 0-1, where 1 = highest risk)\n",
    "# Risk increases with predicted lateness\n",
    "max_late = predicted_days_late.max()\n",
    "min_late = predicted_days_late.min()\n",
    "output_df['risk_score'] = ((predicted_days_late - min_late) / (max_late - min_late)).clip(0, 1)\n",
    "\n",
    "# Add priority flag for strategic accounts that are predicted late\n",
    "output_df['high_priority'] = (\n",
    "    (output_df['is_late'] == 1) & \n",
    "    (output_df.get('STRATEGIC_ACCOUNT', 'No') == 'Yes')\n",
    ").astype(int)\n",
    "\n",
    "# Add prediction timestamp\n",
    "output_df['prediction_date'] = datetime.now()\n",
    "output_df['model_name'] = MODEL_NAME\n",
    "output_df['model_version'] = MODEL_VERSION\n",
    "\n",
    "print(f\"‚úÖ Created output dataframe with {len(output_df):,} predictions\")\n",
    "print(f\"\\nOutput columns: {output_df.shape[1]}\")\n",
    "print(f\"\\nSample predictions:\")\n",
    "output_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8376de0",
   "metadata": {},
   "source": [
    "### üü¶ 9. Save Predictions to Lakehouse\n",
    "\n",
    "Write predictions to a Delta table for Power BI consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a47c4e",
   "metadata": {},
   "source": [
    "**Note:** This table will be available in Power BI via Direct Lake mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(output_df)\n",
    "\n",
    "# Table name\n",
    "table_name = OUTPUT_TABLE\n",
    "\n",
    "print(f\"üíæ Writing predictions to Lakehouse table: {table_name}\")\n",
    "\n",
    "# Write to Lakehouse (overwrite mode with schema update)\n",
    "spark_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"‚úÖ Predictions saved to table: {table_name}\")\n",
    "print(f\"   Rows written: {len(output_df):,}\")\n",
    "print(f\"   Columns: {len(output_df.columns)}\")\n",
    "print(f\"\\nüìä Prediction Summary:\")\n",
    "print(f\"   Total open deliveries: {len(output_df):,}\")\n",
    "print(f\"   Predicted LATE: {output_df['is_late'].sum():,}\")\n",
    "print(f\"   Predicted ON-TIME: {(1-output_df['is_late']).sum():,}\")\n",
    "print(f\"   High priority (Strategic + Late): {output_df['high_priority'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify table was created\n",
    "print(\"\\n=== Verification ===\")\n",
    "verification_df = spark.sql(f\"SELECT * FROM {OUTPUT_TABLE} LIMIT 10\")\n",
    "print(f\"Table '{OUTPUT_TABLE}' contains {verification_df.count()} rows (showing first 10):\")\n",
    "verification_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd2e79",
   "metadata": {},
   "source": [
    "### üü¶ 10. Visualize High-Risk Deliveries\n",
    "\n",
    "Identify deliveries requiring immediate attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high-risk deliveries\n",
    "print(\"=\"*70)\n",
    "print(\"HIGH-RISK LATE DELIVERY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Late deliveries by bucket\n",
    "bucket_counts = output_df['lateness_bucket'].value_counts().sort_index()\n",
    "print(\"\\nüì¶ Deliveries by Lateness Bucket:\")\n",
    "print(bucket_counts)\n",
    "\n",
    "# Late deliveries by strategic account\n",
    "if 'STRATEGIC_ACCOUNT' in output_df.columns:\n",
    "    print(\"\\nüéØ Late Deliveries by Account Type:\")\n",
    "    late_df = output_df[output_df['is_late'] == 1]\n",
    "    strategic_summary = late_df.groupby('STRATEGIC_ACCOUNT').agg({\n",
    "        'Delivery_Number': 'count',\n",
    "        'predicted_days_late': 'mean',\n",
    "        'DELIVERY_VALUE_USD': 'sum'\n",
    "    }).round(2)\n",
    "    strategic_summary.columns = ['Count', 'Avg Days Late', 'Total Value ($)']\n",
    "    print(strategic_summary)\n",
    "\n",
    "# Top 10 highest risk deliveries\n",
    "print(\"\\nüö® TOP 10 HIGHEST RISK DELIVERIES:\")\n",
    "high_risk = output_df.nlargest(10, 'risk_score')[\n",
    "    ['Delivery_Number', 'Brand', 'Channel', 'STRATEGIC_ACCOUNT', \n",
    "     'predicted_days_late', 'lateness_bucket', 'risk_score', 'DELIVERY_VALUE_USD']\n",
    "]\n",
    "print(high_risk.to_string(index=False))\n",
    "\n",
    "# Late deliveries by brand/channel\n",
    "if 'Brand' in output_df.columns and 'Channel' in output_df.columns:\n",
    "    print(\"\\nüìä Late Deliveries by Brand & Channel:\")\n",
    "    late_by_brand_channel = output_df[output_df['is_late'] == 1].groupby(['Brand', 'Channel']).size().reset_index(name='Late_Count')\n",
    "    print(late_by_brand_channel.sort_values('Late_Count', ascending=False).head(15).to_string(index=False))\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06fc1c6",
   "metadata": {},
   "source": [
    "### üü¶ 11. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac0fe1c",
   "metadata": {},
   "source": [
    "#### Prediction Distribution\n",
    "\n",
    "Shows the distribution of predicted aging days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a0274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction distribution histogram\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Prediction distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(output_df['predicted_aging_days'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(output_df['predicted_aging_days'].mean(), color='red', linestyle='--', \n",
    "            label=f\"Mean: {output_df['predicted_aging_days'].mean():.1f}\")\n",
    "plt.axvline(output_df['predicted_aging_days'].median(), color='green', linestyle='--', \n",
    "            label=f\"Median: {output_df['predicted_aging_days'].median():.1f}\")\n",
    "plt.xlabel('Predicted Aging Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predicted Aging Days')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(output_df['predicted_aging_days'], vert=True)\n",
    "plt.ylabel('Predicted Aging Days')\n",
    "plt.title('Predicted Aging Days - Box Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a80f1",
   "metadata": {},
   "source": [
    "#### Actual vs Predicted (if actuals available)\n",
    "\n",
    "Compares predictions to actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'actual_aging_days' in output_df.columns:\n",
    "    valid_df = output_df[output_df['actual_aging_days'].notna()]\n",
    "    \n",
    "    if len(valid_df) > 0:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(valid_df['actual_aging_days'], valid_df['predicted_aging_days'], alpha=0.5)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(valid_df['actual_aging_days'].min(), valid_df['predicted_aging_days'].min())\n",
    "        max_val = max(valid_df['actual_aging_days'].max(), valid_df['predicted_aging_days'].max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "        \n",
    "        plt.xlabel('Actual Aging Days')\n",
    "        plt.ylabel('Predicted Aging Days')\n",
    "        plt.title('Actual vs Predicted Aging Days')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid actual values to compare\")\n",
    "else:\n",
    "    print(\"Actual values not available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d674f841",
   "metadata": {},
   "source": [
    "### üü¶ 12. Power BI Integration Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e53bc69",
   "metadata": {},
   "source": [
    "## ‚úÖ Predictions Complete!\n",
    "\n",
    "Your aging predictions have been saved to the **`aging_predictions`** table in your Lakehouse.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Next Steps: Connect to Power BI\n",
    "\n",
    "#### Option 1: Direct Lake (Recommended)\n",
    "\n",
    "1. Open Power BI Desktop or Power BI Service\n",
    "2. Create a new report\n",
    "3. Connect to your Fabric Lakehouse\n",
    "4. Select the **`aging_predictions`** table\n",
    "5. The table will auto-refresh when you re-run this notebook\n",
    "\n",
    "#### Option 2: Import Mode\n",
    "\n",
    "1. Get Data ‚Üí Lakehouse\n",
    "2. Select **`aging_predictions`** table\n",
    "3. Click **Load**\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Recommended Power BI Visuals\n",
    "\n",
    "**Page 1: Executive Dashboard**\n",
    "- KPI Cards:\n",
    "  - Total Deliveries\n",
    "  - Avg Predicted Aging Days\n",
    "  - Avg Actual Aging Days (if available)\n",
    "  - MAE\n",
    "- Line Chart: Predicted vs Actual over time\n",
    "- Histogram: Distribution of predictions\n",
    "\n",
    "**Page 2: Plant Analysis**\n",
    "- Bar Chart: Avg Aging by Plant\n",
    "- Table: Plant details with predictions\n",
    "- Scatter Plot: Prediction error by Plant\n",
    "\n",
    "**Page 3: Brand & Channel**\n",
    "- Matrix: Brand √ó Channel with avg aging\n",
    "- Bar Chart: Top brands by aging days\n",
    "- Pie Chart: Distribution by channel\n",
    "\n",
    "**Page 4: Detailed Explorer**\n",
    "- Table with filters:\n",
    "  - Delivery Number\n",
    "  - Plant\n",
    "  - Brand\n",
    "  - Channel\n",
    "  - Predicted Aging Days\n",
    "  - Actual Aging Days\n",
    "  - Prediction Error\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Sample DAX Measures\n",
    "\n",
    "Use the DAX measures in `powerbi/dax/measures_basic.dax` and `measures_advanced.dax`.\n",
    "\n",
    "Key measures:\n",
    "```dax\n",
    "Total Deliveries = COUNTROWS(aging_predictions)\n",
    "\n",
    "Avg Predicted Aging = AVERAGE(aging_predictions[predicted_aging_days])\n",
    "\n",
    "Avg Prediction Error = AVERAGE(aging_predictions[absolute_error])\n",
    "\n",
    "Predictions Within 2 Days = \n",
    "CALCULATE(\n",
    "    COUNTROWS(aging_predictions),\n",
    "    aging_predictions[absolute_error] <= 2\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Rerunning Predictions\n",
    "\n",
    "To update predictions with new data:\n",
    "1. Refresh your Power BI semantic model\n",
    "2. Rerun this notebook (03_batch_scoring_pipeline.ipynb)\n",
    "3. Power BI will automatically see the updated predictions\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Your aging prediction pipeline is complete!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
