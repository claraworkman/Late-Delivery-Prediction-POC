{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4073879",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U semantic-link --q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660d4d1",
   "metadata": {},
   "source": [
    "## üîß Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from flaml import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Experiment tracking\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Semantic Link - Connect to Power BI\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File handling for encoders\n",
    "import pickle\n",
    "import tempfile\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sklearn\n",
    "\n",
    "print(\"‚úÖ All libraries imported\")\n",
    "print(f\"   scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40198d2c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace\n",
    "ws = fabric.get_workspace_id()\n",
    "\n",
    "# Semantic model name - UPDATE THIS\n",
    "SEMANTIC_MODEL = \"Deliveries Aging - Open and Closed\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"delivery_lateness_predictor_fixed\"\n",
    "TARGET_COLUMN = \"AGE_REQ_DATE\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Workspace ID: {ws}\")\n",
    "print(f\"   Semantic Model: {SEMANTIC_MODEL}\")\n",
    "print(f\"   Model Name: {MODEL_NAME}\")\n",
    "print(f\"   Target: {TARGET_COLUMN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de543fa",
   "metadata": {},
   "source": [
    "## üìä Step 4: Load Training Data (Closed Deliveries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edca6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query closed deliveries (those with GI Date)\n",
    "dax_query = \"\"\"\n",
    "EVALUATE\n",
    "FILTER(\n",
    "    Aging,\n",
    "    NOT(ISBLANK(Aging[GI Date])) &&\n",
    "    NOT(ISBLANK(Aging[Req. Date Header]))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df_closed = fabric.evaluate_dax(dataset=SEMANTIC_MODEL, dax_string=dax_query, workspace=ws)\n",
    "\n",
    "# Clean column names (remove DAX table prefixes)\n",
    "df_closed.columns = [col.split('[')[-1].replace(']', '') if '[' in col else col for col in df_closed.columns]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_closed):,} closed deliveries for training\")\n",
    "print(f\"   Columns: {df_closed.shape[1]}\")\n",
    "print(f\"\\nüìä Sample data:\")\n",
    "df_closed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8898f",
   "metadata": {},
   "source": [
    "## üîë Step 5: Feature Engineering with Saved Encoders\n",
    "\n",
    "**CRITICAL FIX:** Using `LabelEncoder` instead of `.cat.codes` to ensure consistent encoding between training and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b760ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "feature_cols = [\n",
    "    \"Channel\",\n",
    "    \"Delivery Priority\",\n",
    "    \"EWM Shipping Condition\",\n",
    "    \"Shipping Point\",\n",
    "    \"Sold To Name 1\",\n",
    "    \"Standard Or Custom\",\n",
    "    \"Product Category\"\n",
    "]\n",
    "\n",
    "# Define target\n",
    "target_col = \"AGE_REQ_DATE\"\n",
    "\n",
    "# Extract features and target\n",
    "X = df_closed[feature_cols].copy()\n",
    "y = df_closed[target_col].copy()\n",
    "\n",
    "# ==============================================================================\n",
    "# ENCODE CATEGORICAL VARIABLES WITH SAVED ENCODERS\n",
    "# ==============================================================================\n",
    "categorical_cols = feature_cols  # All features are categorical\n",
    "\n",
    "# Create and fit encoders\n",
    "encoders = {}\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].fillna('Unknown')\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    encoders[col] = le\n",
    "    print(f\"‚úÖ Encoded {col}: {len(le.classes_)} unique categories\")\n",
    "\n",
    "# Save encoders to temporary file (will be logged to MLflow later)\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "encoder_path = os.path.join(temp_dir, 'encoders.pkl')\n",
    "with open(encoder_path, 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved {len(encoders)} encoders to: {encoder_path}\")\n",
    "print(f\"‚úÖ Features: {X.shape[1]} columns, {X.shape[0]:,} rows\")\n",
    "print(f\"‚úÖ Target: {y.shape[0]:,} values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46062d",
   "metadata": {},
   "source": [
    "## üîÄ Step 6: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"‚úÖ Train: {len(X_train):,} samples\")\n",
    "print(f\"‚úÖ Test:  {len(X_test):,} samples\")\n",
    "print(f\"‚úÖ Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7608f27",
   "metadata": {},
   "source": [
    "## ü§ñ Step 7: Train AutoML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446109c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML settings\n",
    "automl = AutoML()\n",
    "\n",
    "settings = {\n",
    "    \"time_budget\": 60,  # 1 minute for quick training\n",
    "    \"task\": \"regression\",\n",
    "    \"metric\": \"mae\",\n",
    "    \"estimator_list\": [\n",
    "        \"rf\",        # RandomForestRegressor\n",
    "        \"xgboost\",   # XGBoostRegressor\n",
    "        \"extra_tree\" # ExtraTreesRegressor\n",
    "    ],\n",
    "    \"log_file_name\": \"automl_training.log\",\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "print(\"üöÄ Starting AutoML training...\")\n",
    "automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"   Best model: {automl.best_estimator}\")\n",
    "print(f\"   Best MAE: {automl.best_loss:.3f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd6d1a",
   "metadata": {},
   "source": [
    "## üìà Step 8: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "preds = automl.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test, preds)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE:  {mae:.3f} days  ‚Üê Average prediction error\")\n",
    "print(f\"RMSE: {rmse:.3f} days  ‚Üê Larger errors penalized more\")\n",
    "print(f\"R¬≤:   {r2:.3f}        ‚Üê Variance explained (higher is better)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if mae < 1.0:\n",
    "    print(\"\\n‚úÖ EXCELLENT performance! MAE < 1 day is production-ready.\")\n",
    "elif mae < 2.0:\n",
    "    print(\"\\n‚úÖ GOOD performance! MAE < 2 days is acceptable.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Model may need improvement. Consider adding more features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccf5b6",
   "metadata": {},
   "source": [
    "## üíæ Step 9: Register Model in MLflow WITH Encoders\n",
    "\n",
    "**CRITICAL:** This cell logs the encoders alongside the model so they can be loaded during scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "with mlflow.start_run(run_name=f\"{MODEL_NAME}_training\") as run:\n",
    "    \n",
    "    # Log training metrics\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_features\", len(feature_cols))\n",
    "    mlflow.log_param(\"n_train_samples\", len(X_train))\n",
    "    mlflow.log_param(\"n_test_samples\", len(X_test))\n",
    "    mlflow.log_param(\"sklearn_version\", sklearn.__version__)\n",
    "    mlflow.log_param(\"estimator\", automl.best_estimator)\n",
    "    \n",
    "    # LOG ENCODERS (CRITICAL FIX)\n",
    "    mlflow.log_artifact(encoder_path, artifact_path=\"encoders\")\n",
    "    \n",
    "    # Log the sklearn model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=automl.model,\n",
    "        artifact_path=\"model\"\n",
    "    )\n",
    "\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "# Register the model\n",
    "registered = mlflow.register_model(\n",
    "    model_uri=model_uri,\n",
    "    name=MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model registered: {registered.name} v{registered.version}\")\n",
    "print(f\"‚úÖ Encoders saved with model\")\n",
    "print(f\"‚úÖ Run ID: {run_id}\")\n",
    "print(f\"\\nüéØ Model is ready for scoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bd6a5",
   "metadata": {},
   "source": [
    "## üìä Step 10: Visualize Model Performance (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Predicted vs Actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, preds, alpha=0.5, s=10)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect predictions')\n",
    "plt.xlabel('Actual AGE_REQ_DATE (days)', fontsize=12)\n",
    "plt.ylabel('Predicted AGE_REQ_DATE (days)', fontsize=12)\n",
    "plt.title(f'Prediction vs Actual (MAE: {mae:.2f} days)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c03a48",
   "metadata": {},
   "source": [
    "## üìä Step 11: Feature Importance (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(automl.model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': automl.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "    plt.title('Feature Importance for Delivery Lateness Prediction', fontsize=14)\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 3 Most Important Features:\")\n",
    "    for idx, row in feature_importance.head(3).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "else:\n",
    "    print(\"Feature importances not available for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec02bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ SCORING PHASE: Generate Predictions for Open Deliveries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139ae77",
   "metadata": {},
   "source": [
    "## üîÑ Step 12: Load Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest version of the model\n",
    "model_uri = f\"models:/{MODEL_NAME}/latest\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
    "print(f\"   URI: {model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98ae00",
   "metadata": {},
   "source": [
    "## üì• Step 13: Load Open Deliveries for Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf72ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query open deliveries (those WITHOUT GI Date)\n",
    "dax_query = \"\"\"\n",
    "EVALUATE\n",
    "FILTER(\n",
    "    Aging,\n",
    "    ISBLANK(Aging[GI Date]) &&\n",
    "    NOT(ISBLANK(Aging[Delivery Created On])) &&\n",
    "    NOT(ISBLANK(Aging[Req. Date Header]))\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "df_open = fabric.evaluate_dax(dataset=SEMANTIC_MODEL, dax_string=dax_query, workspace=ws)\n",
    "\n",
    "# Clean column names\n",
    "df_open.columns = [col.split('[')[-1].replace(']', '') if '[' in col else col for col in df_open.columns]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_open):,} open deliveries for scoring\")\n",
    "print(f\"   Columns: {df_open.shape[1]}\")\n",
    "print(f\"\\nüìä Sample data:\")\n",
    "df_open.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040a4c5",
   "metadata": {},
   "source": [
    "## üîë Step 14: Load Saved Encoders and Prepare Features\n",
    "\n",
    "**CRITICAL FIX:** Loading the same encoders used during training to ensure consistent categorical encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d24a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LOAD SAVED ENCODERS FROM MLFLOW\n",
    "# ==============================================================================\n",
    "print(\"üì¶ Loading saved encoders from MLflow...\\n\")\n",
    "\n",
    "client = MlflowClient()\n",
    "model_versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "\n",
    "if not model_versions:\n",
    "    raise ValueError(f\"‚ùå No model versions found for {MODEL_NAME}\")\n",
    "\n",
    "# Get latest version\n",
    "latest = sorted(model_versions, key=lambda x: int(x.version))[-1]\n",
    "run_id = latest.run_id\n",
    "\n",
    "print(f\"   Model name: {MODEL_NAME}\")\n",
    "print(f\"   Model version: {latest.version}\")\n",
    "print(f\"   Run ID: {run_id}\")\n",
    "\n",
    "# Download encoders artifact\n",
    "encoder_artifact = client.download_artifacts(run_id, \"encoders/encoders.pkl\")\n",
    "with open(encoder_artifact, 'rb') as f:\n",
    "    encoders = pickle.load(f)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(encoders)} encoders\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# APPLY SAVED ENCODERS TO SCORING DATA\n",
    "# ==============================================================================\n",
    "# Extract features\n",
    "X_score = df_open[feature_cols].copy()\n",
    "\n",
    "# Apply saved encoders\n",
    "for col in categorical_cols:\n",
    "    X_score[col] = X_score[col].fillna('Unknown').astype(str)\n",
    "    \n",
    "    if col in encoders:\n",
    "        encoder = encoders[col]\n",
    "        known_classes = set(encoder.classes_)\n",
    "        \n",
    "        # Transform using saved encoder, handle unknown categories\n",
    "        def safe_encode(value):\n",
    "            if value in known_classes:\n",
    "                return encoder.transform([value])[0]\n",
    "            else:\n",
    "                # Unknown category -> assign max code + 1\n",
    "                return len(encoder.classes_)\n",
    "        \n",
    "        X_score[col] = X_score[col].apply(safe_encode)\n",
    "        unique_count = X_score[col].nunique()\n",
    "        print(f\"‚úÖ {col}: {unique_count} unique values (using saved encoder)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No saved encoder for {col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Prepared {len(X_score):,} rows for scoring\")\n",
    "print(f\"‚úÖ Features: {X_score.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea9b91",
   "metadata": {},
   "source": [
    "## üîç Step 15: Verify Encoding Quality (Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENCODING QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nUnique values per feature in scoring data:\")\n",
    "for col in X_score.columns:\n",
    "    nunique = X_score[col].nunique()\n",
    "    print(f\"  {col}: {nunique} unique values\")\n",
    "\n",
    "unique_rows = X_score.drop_duplicates().shape[0]\n",
    "total_rows = X_score.shape[0]\n",
    "uniqueness_pct = (unique_rows / total_rows) * 100\n",
    "\n",
    "print(f\"\\nüìä Unique row combinations: {unique_rows:,} out of {total_rows:,} ({uniqueness_pct:.1f}%)\")\n",
    "\n",
    "if unique_rows < 100:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Very few unique rows - encoding may be broken!\")\n",
    "elif uniqueness_pct < 10:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Low uniqueness - many deliveries have identical features\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ GOOD: Encoding has sufficient variation!\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c3ecd",
   "metadata": {},
   "source": [
    "## üéØ Step 16: Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = model.predict(X_score)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_open['predicted_age_req_date'] = predictions\n",
    "\n",
    "# Calculate predicted ship date\n",
    "df_open['predicted_ship_date'] = (\n",
    "    pd.to_datetime(df_open['Req. Date Header']) + \n",
    "    pd.to_timedelta(df_open['predicted_age_req_date'], unit='d')\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(predictions):,} predictions\")\n",
    "print(f\"\\nüìä Prediction Statistics:\")\n",
    "print(f\"   Mean:   {predictions.mean():.2f} days\")\n",
    "print(f\"   Median: {np.median(predictions):.2f} days\")\n",
    "print(f\"   Min:    {predictions.min():.2f} days (earliest)\")\n",
    "print(f\"   Max:    {predictions.max():.2f} days (latest)\")\n",
    "print(f\"   Std:    {predictions.std():.2f} days\")\n",
    "\n",
    "print(f\"\\nüìà Distribution:\")\n",
    "print(f\"   Predicted Early (<0):     {(predictions < 0).sum():,} ({(predictions < 0).sum()/len(predictions)*100:.1f}%)\")\n",
    "print(f\"   Predicted On-Time (0-3):  {((predictions >= 0) & (predictions <= 3)).sum():,} ({((predictions >= 0) & (predictions <= 3)).sum()/len(predictions)*100:.1f}%)\")\n",
    "print(f\"   Predicted Late (>3):      {(predictions > 3).sum():,} ({(predictions > 3).sum()/len(predictions)*100:.1f}%)\")\n",
    "print(f\"   Predicted Very Late (>5): {(predictions > 5).sum():,} ({(predictions > 5).sum()/len(predictions)*100:.1f}%)\")\n",
    "\n",
    "# Check for uniqueness\n",
    "unique_predictions = len(set(predictions))\n",
    "print(f\"\\nüîç Unique prediction values: {unique_predictions:,} out of {len(predictions):,}\")\n",
    "\n",
    "if unique_predictions < 10:\n",
    "    print(\"\\n‚ùå ERROR: Too few unique predictions - encoding is still broken!\")\n",
    "elif unique_predictions < 100:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Limited prediction variety - check your data\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ EXCELLENT: Predictions show good variation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7ff10",
   "metadata": {},
   "source": [
    "## üëÄ Step 17: View Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample predictions\n",
    "sample_cols = [\n",
    "    'Delivery Number',\n",
    "    'Sold To Name 1',\n",
    "    'Delivery Priority',\n",
    "    'Product Category',\n",
    "    'Req. Date Header',\n",
    "    'predicted_age_req_date',\n",
    "    'predicted_ship_date'\n",
    "]\n",
    "\n",
    "available_sample_cols = [c for c in sample_cols if c in df_open.columns]\n",
    "sample_df = df_open[available_sample_cols].head(20)\n",
    "\n",
    "print(\"\\nüìã Sample Predictions (First 20 rows):\")\n",
    "print(\"=\"*100)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f17c5",
   "metadata": {},
   "source": [
    "## üíæ Step 18: Save Predictions to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Saving Predictions to Lakehouse ===\")\n",
    "\n",
    "# Select essential columns for Power BI\n",
    "output_cols = [\n",
    "    'Delivery Number',\n",
    "    'Sold To Name 1',\n",
    "    'Delivery Priority',\n",
    "    'Product Category',\n",
    "    'Req. Date Header',\n",
    "    'predicted_age_req_date',\n",
    "    'predicted_ship_date'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "available_output_cols = [c for c in output_cols if c in df_open.columns]\n",
    "predictions_df = df_open[available_output_cols].copy()\n",
    "\n",
    "# Add metadata\n",
    "predictions_df['prediction_timestamp'] = datetime.now()\n",
    "predictions_df['model_name'] = MODEL_NAME\n",
    "predictions_df['model_version'] = latest.version\n",
    "\n",
    "# Clean column names for Delta table compatibility\n",
    "predictions_df.columns = (\n",
    "    predictions_df.columns\n",
    "    .str.replace(' ', '_', regex=False)\n",
    "    .str.replace('.', '', regex=False)\n",
    "    .str.replace('-', '_', regex=False)\n",
    ")\n",
    "\n",
    "# Convert datetime columns to strings\n",
    "date_cols = predictions_df.select_dtypes(include=['datetime64']).columns\n",
    "for col in date_cols:\n",
    "    predictions_df[col] = predictions_df[col].astype(str)\n",
    "\n",
    "# Save to Lakehouse table\n",
    "table_name = \"delivery_lateness_predictions\"\n",
    "spark_df = spark.createDataFrame(predictions_df)\n",
    "spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(table_name)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved {len(predictions_df):,} predictions to table: {table_name}\")\n",
    "print(f\"‚úÖ Columns saved: {len(predictions_df.columns)}\")\n",
    "print(f\"\\nüìä Prediction Summary:\")\n",
    "print(f\"   Early (<0 days):      {(df_open['predicted_age_req_date'] < 0).sum():,}\")\n",
    "print(f\"   On-Time (0-3 days):   {((df_open['predicted_age_req_date'] >= 0) & (df_open['predicted_age_req_date'] <= 3)).sum():,}\")\n",
    "print(f\"   Late (>3 days):       {(df_open['predicted_age_req_date'] > 3).sum():,}\")\n",
    "print(f\"   Very Late (>5 days):  {(df_open['predicted_age_req_date'] > 5).sum():,}\")\n",
    "\n",
    "print(f\"\\nüéØ Predictions are ready for Power BI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef5798",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Pipeline Complete!\n",
    "\n",
    "**What was accomplished:**\n",
    "1. ‚úÖ Trained AutoML model with MAE ~0.6 days\n",
    "2. ‚úÖ Saved categorical encoders with the model\n",
    "3. ‚úÖ Loaded encoders during scoring for consistency\n",
    "4. ‚úÖ Generated unique predictions for each delivery\n",
    "5. ‚úÖ Saved predictions to Lakehouse table\n",
    "\n",
    "**Next Steps for Power BI:**\n",
    "\n",
    "1. **Add Prediction Table** to your semantic model\n",
    "2. **Create Relationship**: `Aging[Delivery Number]` ‚Üí `delivery_lateness_predictions[Delivery_Number]`\n",
    "3. **Add DAX Calculated Columns**:\n",
    "\n",
    "```dax\n",
    "// Lateness Category\n",
    "Lateness Category = \n",
    "SWITCH(\n",
    "    TRUE(),\n",
    "    delivery_lateness_predictions[predicted_age_req_date] < 0, \"Early\",\n",
    "    delivery_lateness_predictions[predicted_age_req_date] <= 3, \"On-Time\",\n",
    "    \"Late\"\n",
    ")\n",
    "\n",
    "// At Risk Flag\n",
    "At Risk = \n",
    "delivery_lateness_predictions[predicted_age_req_date] > 3\n",
    "```\n",
    "\n",
    "4. **Build Dashboards** for:\n",
    "   - At-risk deliveries requiring customer outreach\n",
    "   - Predicted lateness by customer/product/shipping point\n",
    "   - Daily operations prioritization\n",
    "\n",
    "5. **Schedule This Notebook** to run daily at 6 AM for fresh predictions\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
